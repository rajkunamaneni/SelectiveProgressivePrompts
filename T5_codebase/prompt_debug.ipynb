{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "37338f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import logging, os, argparse\n",
    "\n",
    "import t5_model, t5_dataset\n",
    "from copy import deepcopy\n",
    "from transformers import AdamW\n",
    "\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c08b52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResMLP(torch.nn.Module):\n",
    "    def __init__(self, bottleneck_size,\n",
    "                 module_type='MLP1',\n",
    "                 emb_dimension=512,\n",
    "                 residual=True,\n",
    "                 layer_norm=True):\n",
    "        super().__init__()\n",
    "        if module_type=='MLP1':\n",
    "            if layer_norm:\n",
    "                self.module = nn.Sequential(\n",
    "                    nn.Linear(emb_dimension, bottleneck_size),\n",
    "                    #nn.ReLU(),\n",
    "                    nn.Tanh(),\n",
    "                    nn.Linear(bottleneck_size, emb_dimension),\n",
    "                    nn.LayerNorm(emb_dimension),\n",
    "                )\n",
    "            else:\n",
    "                self.module = nn.Sequential(\n",
    "                    nn.Linear(emb_dimension, bottleneck_size),\n",
    "                    #nn.ReLU(),\n",
    "                    nn.Tanh(),\n",
    "                    nn.Linear(bottleneck_size, emb_dimension),\n",
    "                )\n",
    "\n",
    "        elif module_type=='MLP2':\n",
    "            self.module = nn.Sequential(\n",
    "                nn.Linear(emb_dimension, bottleneck_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(bottleneck_size, bottleneck_size),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(bottleneck_size, emb_dimension),\n",
    "                nn.LayerNorm(emb_dimension),\n",
    "            )\n",
    "        self.residual = residual\n",
    "        if self.residual:\n",
    "            print('Using skip connection in MLP')\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        if self.residual:\n",
    "            return self.module(inputs) + inputs\n",
    "        else:\n",
    "            return self.module(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf722bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step_lester(trainer, batch, prompt, embed_prompt=False):\n",
    "    model = trainer.model\n",
    "    if embed_prompt:\n",
    "        mlp = model.mlp\n",
    "    tokenizer = trainer.tokenizer\n",
    "    prefix_MLP = trainer.prefix_MLP\n",
    "    prefix_len = trainer.prefix_len\n",
    "\n",
    "    batch = {k: batch[k].to(trainer.device) for k in batch}\n",
    "    lm_labels = batch[\"target_ids\"]\n",
    "    lm_labels[lm_labels[:, :] == tokenizer.pad_token_id] = -100\n",
    "\n",
    "    inputs_embeds = model.encoder.embed_tokens(batch[\"source_ids\"])\n",
    "    #inputs_embeds[:, -prefix_len:, :] = prefix_MLP(inputs_embeds[:, -prefix_len:, :].clone().to(self.device))\n",
    "    k = inputs_embeds.shape[0]\n",
    "    if embed_prompt:\n",
    "        #prompt = model.prompt_layer_norm(prompt)\n",
    "        prompt = mlp(prompt)\n",
    "    inputs_embeds = torch.concat([prompt.repeat(k, 1, 1), \n",
    "                                  inputs_embeds], axis=1)[:,:512]\n",
    "    \n",
    "    source_mask_updated = torch.concat( (batch[\"source_mask\"][0][0].repeat(k,5), \n",
    "                                         batch[\"source_mask\"]), axis=1)[:,:512]\n",
    "    #source_mask_updated = batch[\"source_mask\"][0][0].repeat(k,512)\n",
    "    \n",
    "    encoder_outputs = model.encoder(\n",
    "                            #input_ids=batch[\"source_ids\"],\n",
    "                            attention_mask=source_mask_updated, #batch[\"source_mask\"],\n",
    "                            #labels=lm_labels,\n",
    "                            #decoder_attention_mask=batch['target_mask']\n",
    "                            #input_ids=input_ids,\n",
    "                            #attention_mask=attention_mask,\n",
    "                            inputs_embeds=inputs_embeds,\n",
    "                            head_mask=None, #head_mask,\n",
    "                            output_attentions=None, #output_attentions,\n",
    "                            output_hidden_states=None, #output_hidden_states,\n",
    "                            return_dict=None, #return_dict,\n",
    "                        )\n",
    "\n",
    "    outputs = model(\n",
    "        input_ids=batch[\"source_ids\"],\n",
    "        attention_mask=source_mask_updated, #batch[\"source_mask\"],\n",
    "        labels=lm_labels,\n",
    "        decoder_attention_mask=batch['target_mask'],\n",
    "        encoder_outputs=encoder_outputs,\n",
    "    )\n",
    "    loss = outputs[0]\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "def validate_lester(trainer, dataloader_val, task,\n",
    "                    class_keys=['equivalent', 'different'],\n",
    "                    max_length=2, print_outpust=False,\n",
    "                    embed_prompt=False):\n",
    "    model = trainer.model\n",
    "    if embed_prompt:\n",
    "        mlp = model.mlp\n",
    "        #mlp.eval()  \n",
    "    prompt = model.prompt\n",
    "    tokenizer = trainer.tokenizer\n",
    "    \n",
    "    #prefix_len = trainer.prefix_len\n",
    "    #N = model.encoder.embed_tokens.weight.shape[0] - prefix_len\n",
    "    model.eval()\n",
    "\n",
    "    corr, total = 0, 0\n",
    "    try:\n",
    "        metric = datasets.load_metric('glue', task)\n",
    "    except:\n",
    "        metric = datasets.load_metric('accuracy')\n",
    "\n",
    "\n",
    "    for i, batch in enumerate(tqdm(dataloader_val)):\n",
    "        batch = {k:batch[k].to(trainer.device) for k in batch}\n",
    "        # batch['source_ids'] = torch.concat([prefix[:batch['source_ids'].shape[0]],\n",
    "        #                                     batch['source_ids'].to(self.device)], axis=1)[:,:512]\n",
    "\n",
    "        inputs_embeds = model.encoder.embed_tokens(batch[\"source_ids\"]).to(trainer.device)\n",
    "        #inputs_embeds[:, -prefix_len:, :] = prefix_MLP(inputs_embeds[:, -prefix_len:, :].clone().to(self.device))\n",
    "        k = inputs_embeds.shape[0]\n",
    "        \n",
    "        if embed_prompt:\n",
    "            #prompt = model.prompt_layer_norm(prompt)\n",
    "            prompt = mlp(prompt)\n",
    "        \n",
    "        inputs_embeds = torch.concat([prompt.repeat(k, 1, 1),\n",
    "                                      inputs_embeds], axis=1)[:,:512]\n",
    "        \n",
    "        source_mask_updated = torch.concat( (batch[\"source_mask\"][0][0].repeat(k,5), \n",
    "                                             batch[\"source_mask\"]), axis=1)[:,:512]\n",
    "        #source_mask_updated = batch[\"source_mask\"][0][0].repeat(k,512)\n",
    "        \n",
    "\n",
    "        encoder_outputs = model.encoder(\n",
    "                                #input_ids=batch[\"source_ids\"],\n",
    "                                #attention_mask=batch[\"source_mask\"],\n",
    "                                attention_mask=source_mask_updated,\n",
    "            \n",
    "                                #labels=lm_labels,\n",
    "                                #decoder_attention_mask=batch['target_mask']\n",
    "                                #input_ids=input_ids,\n",
    "                                #attention_mask=attention_mask,\n",
    "                                inputs_embeds=inputs_embeds,\n",
    "                                head_mask=None, #head_mask,\n",
    "                                output_attentions=None, #output_attentions,\n",
    "                                output_hidden_states=None, #output_hidden_states,\n",
    "                                return_dict=None, #return_dict,\n",
    "                            )\n",
    "\n",
    "        outs = model.generate(\n",
    "            input_ids=batch[\"source_ids\"],\n",
    "            #attention_mask=batch[\"source_mask\"],\n",
    "            attention_mask=source_mask_updated,\n",
    "            #labels=lm_labels,\n",
    "            #decoder_attention_mask=batch['target_mask'],\n",
    "            encoder_outputs=encoder_outputs,\n",
    "            max_length=max_length,\n",
    "        )\n",
    "        dec = [tokenizer.decode(ids) for ids in outs]\n",
    "\n",
    "        texts = [tokenizer.decode(ids) for ids in batch['source_ids']]\n",
    "        targets = [tokenizer.decode(ids) for ids in batch['target_ids']]\n",
    "\n",
    "        #print(dec, texts, targets)\n",
    "        corr += np.sum([trainer.process_str(x)==trainer.process_str(y) for x,y in zip(dec, targets)])\n",
    "        total += batch['source_ids'].shape[0]\n",
    "\n",
    "        if i<10 and print_outpust:\n",
    "            print(dec)\n",
    "            print(targets)\n",
    "\n",
    "        # CHANGE FOR MULTI CLASS!!!\n",
    "        metric.add_batch(predictions=[1 if class_keys[1] in x else 0 for x in dec],\n",
    "                         references=[1 if class_keys[1] in x else 0 for x in targets])\n",
    "\n",
    "        # computing loss\n",
    "#         lm_labels = batch[\"target_ids\"]\n",
    "#         lm_labels[lm_labels[:, :] == self.tokenizer.pad_token_id] = -100\n",
    "\n",
    "#         outputs = model(\n",
    "#             input_ids=batch[\"source_ids\"],\n",
    "#             attention_mask=batch[\"source_mask\"],\n",
    "#             labels=lm_labels,\n",
    "#             decoder_attention_mask=batch['target_mask'],\n",
    "#             encoder_outputs=encoder_outputs,\n",
    "#         )\n",
    "#         loss = outputs[0].detach().cpu().numpy()\n",
    "#         loss_total.append(loss)\n",
    "\n",
    "\n",
    "    return corr/total, metric.compute()#, np.mean(loss_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecffc49f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53134917",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(trainer, prompt_len):\n",
    "    model = trainer.model\n",
    "    N = model.encoder.embed_tokens.weight.shape[0]\n",
    "    prompt_weigths = []\n",
    "\n",
    "    for i in range(prompt_len):\n",
    "        with torch.no_grad():\n",
    "            j = np.random.randint(N)\n",
    "            #j = 21\n",
    "            w = deepcopy(model.encoder.embed_tokens.weight[j].detach().cpu().numpy())\n",
    "            prompt_weigths.append(w)\n",
    "    prompt_weigths = np.array(prompt_weigths)\n",
    "    return prompt_weigths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59351c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c640c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing weights\n",
      "Using AdamW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/arazdai/miniconda/envs/nlp/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# save_path = os.path.join(args.save_dir, args.save_name)\n",
    "#     if not os.path.exists(save_path):\n",
    "#         os.mkdir(save_path)\n",
    "\n",
    "TrainerT5= t5_model.PromptModelT5(model_name='t5-small',\n",
    "                                  prefix_len=0,\n",
    "                                  freeze_weights=True,\n",
    "                                  freeze_except='xxxshared', # freeze all weights\n",
    "                                  lr=0.3,\n",
    "                                  weight_decay=0.00,\n",
    "                                  prompt_name='PRE',\n",
    "                                  prefix_MLP='None', # using custom prefix MLP\n",
    "                                  #mlp_bottleneck=args.mlp_bottleneck,\n",
    "                                  #weight_decay_mlp=0.0,\n",
    "                                  #mlp_lr=args.lr_mlp,\n",
    "                                  #mlp_layer_norm=args.mlp_layer_norm==1,\n",
    "                                  early_stopping=False,\n",
    "                                  #opt=args.optimizer,\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bd10c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created prompt:  (5, 512)\n"
     ]
    }
   ],
   "source": [
    "prompt_weigths = get_prompt(TrainerT5, prompt_len=5)\n",
    "TrainerT5.model.prompt = nn.Parameter(torch.tensor(prompt_weigths, requires_grad=True))\n",
    "print('created prompt: ', prompt_weigths.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfd018dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ -4.9688,  -3.1250,  17.8750,  ...,   3.6562,  -7.2812,  32.0000],\n",
       "        [ -7.9375,  -6.5000, -59.5000,  ..., -66.5000,  24.2500,  17.0000],\n",
       "        [-52.0000, -15.5000,  18.5000,  ..., -35.0000,  20.6250,   6.6250],\n",
       "        [ 15.0000, -34.7500,  15.4375,  ..., -30.3750,  56.5000,  16.3750],\n",
       "        [ 15.1875,  -5.1562, -33.2500,  ...,   8.2500, -58.7500,  28.2500]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TrainerT5.model.prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "184a0c24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -4.96875,  -3.125  ,  17.875  , ...,   3.65625,  -7.28125,\n",
       "         32.     ],\n",
       "       [ -7.9375 ,  -6.5    , -59.5    , ..., -66.5    ,  24.25   ,\n",
       "         17.     ],\n",
       "       [-52.     , -15.5    ,  18.5    , ..., -35.     ,  20.625  ,\n",
       "          6.625  ],\n",
       "       [ 15.     , -34.75   ,  15.4375 , ..., -30.375  ,  56.5    ,\n",
       "         16.375  ],\n",
       "       [ 15.1875 ,  -5.15625, -33.25   , ...,   8.25   , -58.75   ,\n",
       "         28.25   ]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_weigths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f992095",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdamW (\n",
       "Parameter Group 0\n",
       "    betas: (0.9, 0.999)\n",
       "    correct_bias: True\n",
       "    eps: 1e-08\n",
       "    lr: 0.3\n",
       "    weight_decay: 1e-05\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in TrainerT5.model.named_parameters()],\n",
    "        \"weight_decay\": 1e-5,\n",
    "        \"lr\": 0.3,\n",
    "    }\n",
    "]\n",
    "TrainerT5.optimizer = AdamW(optimizer_grouped_parameters, eps=1e-8)\n",
    "TrainerT5.optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df46b926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, p in TrainerT5.model.named_parameters():\n",
    "#     if p.requires_grad:\n",
    "#         print(p, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0e15ad8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2560"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytorch_total_params = sum(p.numel() for p in TrainerT5.model.parameters() if p.requires_grad)\n",
    "pytorch_total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f072f55d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3cb99f10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset super_glue (/data/home/arazdai/.cache/huggingface/datasets/super_glue/boolq/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7)\n",
      "Loading cached shuffled indices for dataset at /data/home/arazdai/.cache/huggingface/datasets/super_glue/boolq/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7/cache-20bba0ff54488a0d.arrow\n",
      "Parameter 'function'=<function T5Dataset.get_final_ds.<locals>.<lambda> at 0x7fca33b3e5e0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "105f0ff215464ac5be753f22d1cb6d47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9427 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/arazdai/miniconda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:194: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/data/home/arazdai/miniconda/envs/nlp/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Reusing dataset super_glue (/data/home/arazdai/.cache/huggingface/datasets/super_glue/boolq/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7)\n",
      "Loading cached shuffled indices for dataset at /data/home/arazdai/.cache/huggingface/datasets/super_glue/boolq/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7/cache-56923150e227aa8a.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94bd5ece87194359af150abc24e669c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3270 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "task = 'boolq'\n",
    "target_len = 2\n",
    "if task=='rte' or task=='mrpc': target_len=5\n",
    "\n",
    "ds2 = t5_dataset.T5Dataset(TrainerT5.tokenizer, task)\n",
    "dataloader_train = ds2.get_final_ds(task, 'train', batch_size=8, k=-1,\n",
    "                                    target_len=target_len, prefix_list=[])\n",
    "\n",
    "k_val = -1 #if (args.select_k_per_class==-1 or task in ['mrpc', 'rte']) else int(0.2*args.select_k_per_class)\n",
    "dataloader_val = ds2.get_final_ds(task, 'validation',\n",
    "                                  batch_size=8, k=k_val, return_test=False,\n",
    "                                  target_len=target_len, prefix_list=[])\n",
    "\n",
    "class_keys = ds2.task_to_labels[task]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2eb101",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "28380f35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task =  boolq\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27c374025e9f4960828e3e2bd27a97ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1179 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f271b75bd5b4e7fbe10bdb6ba9b3b65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/409 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -> 0.6253822629969419 {'accuracy': 0.6253822629969419}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ead2a13472640f58453620f4b18e640",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1179 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be41129143e74643a2d3a3ad1d0c4c92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/409 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 -> 0.6275229357798165 {'accuracy': 0.6275229357798165}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "846e49236e4d4d72a263cfcd2b34946d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1179 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m batch \u001b[38;5;241m=\u001b[39m {k:batch[k]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m batch}\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#lim = batch['source_ids'].shape[0]\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step_lester\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTrainerT5\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTrainerT5\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membed_prompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     21\u001b[0m TrainerT5\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mtrain_step_lester\u001b[0;34m(trainer, batch, prompt, embed_prompt)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#source_mask_updated = batch[\"source_mask\"][0][0].repeat(k,512)\u001b[39;00m\n\u001b[1;32m     26\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[1;32m     27\u001b[0m                         \u001b[38;5;66;03m#input_ids=batch[\"source_ids\"],\u001b[39;00m\n\u001b[1;32m     28\u001b[0m                         attention_mask\u001b[38;5;241m=\u001b[39msource_mask_updated, \u001b[38;5;66;03m#batch[\"source_mask\"],\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m                         return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;66;03m#return_dict,\u001b[39;00m\n\u001b[1;32m     38\u001b[0m                     )\n\u001b[0;32m---> 40\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msource_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource_mask_updated\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#batch[\"source_mask\"],\u001b[39;49;00m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlm_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtarget_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/miniconda/envs/nlp/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda/envs/nlp/lib/python3.9/site-packages/transformers/adapters/context.py:103\u001b[0m, in \u001b[0;36mForwardContext.wrap.<locals>.wrapper_func\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39madapters \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 103\u001b[0m         results \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py:1692\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1689\u001b[0m         decoder_attention_mask \u001b[38;5;241m=\u001b[39m decoder_attention_mask\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mfirst_device)\n\u001b[1;32m   1691\u001b[0m \u001b[38;5;66;03m# Decode\u001b[39;00m\n\u001b[0;32m-> 1692\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1693\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1694\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1695\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1696\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1697\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1698\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1699\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1700\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1701\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1702\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1703\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1704\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1705\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1707\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m decoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1709\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/envs/nlp/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py:1066\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1053\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m checkpoint(\n\u001b[1;32m   1054\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m   1055\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1063\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# past_key_value is always None with gradient checkpointing\u001b[39;00m\n\u001b[1;32m   1064\u001b[0m     )\n\u001b[1;32m   1065\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1066\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1067\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1068\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1069\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1070\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1071\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1072\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1073\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1074\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1075\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1076\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[38;5;66;03m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[1;32m   1081\u001b[0m \u001b[38;5;66;03m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[1;32m   1082\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda/envs/nlp/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py:721\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    719\u001b[0m     query_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 721\u001b[0m cross_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_value_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    727\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    729\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    730\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    731\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    732\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    734\u001b[0m \u001b[38;5;66;03m# clamp inf values to enable fp16 training\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/envs/nlp/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py:642\u001b[0m, in \u001b[0;36mT5LayerCrossAttention.forward\u001b[0;34m(self, hidden_states, key_value_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, query_length, output_attentions)\u001b[0m\n\u001b[1;32m    630\u001b[0m normed_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(hidden_states)\n\u001b[1;32m    631\u001b[0m attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mEncDecAttention(\n\u001b[1;32m    632\u001b[0m     normed_hidden_states,\n\u001b[1;32m    633\u001b[0m     mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    640\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    641\u001b[0m )\n\u001b[0;32m--> 642\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madapter_layer_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    643\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m attention_output[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n\u001b[1;32m    644\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/miniconda/envs/nlp/lib/python3.9/site-packages/transformers/adapters/layer.py:442\u001b[0m, in \u001b[0;36mAdapterLayer.adapter_layer_forward\u001b[0;34m(self, hidden_states, input_tensor, layer_norm)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madapter_layer_forward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states, input_tensor, layer_norm):\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;124;03m    Called for each forward pass through adapters.\u001b[39;00m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 442\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mis_adaptable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m:\n\u001b[1;32m    443\u001b[0m         \u001b[38;5;66;03m# First check current context before falling back to defined setup\u001b[39;00m\n\u001b[1;32m    444\u001b[0m         context \u001b[38;5;241m=\u001b[39m AdapterSetup\u001b[38;5;241m.\u001b[39mget_context()\n\u001b[1;32m    445\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda/envs/nlp/lib/python3.9/site-packages/transformers/configuration_utils.py:250\u001b[0m, in \u001b[0;36mPretrainedConfig.__getattribute__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[0;32m--> 250\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattribute_map\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattribute_map\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    251\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattribute_map\u001b[39m\u001b[38;5;124m\"\u001b[39m)[key]\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# previous training script\n",
    "batch_size =8\n",
    "\n",
    "print('task = ', task)\n",
    "model = TrainerT5.model\n",
    "model.to('cuda')\n",
    "\n",
    "embed_prompt = False\n",
    "\n",
    "for epoch in range(50):\n",
    "\n",
    "    model.train() \n",
    "    #mlp.train() \n",
    "        \n",
    "    for i, batch in enumerate(tqdm(dataloader_train)):\n",
    "        batch = {k:batch[k].to('cuda') for k in batch}\n",
    "        #lim = batch['source_ids'].shape[0]\n",
    "        loss = train_step_lester(TrainerT5, batch, TrainerT5.model.prompt, embed_prompt=embed_prompt)\n",
    "        loss.backward()\n",
    "\n",
    "        TrainerT5.optimizer.step()\n",
    "        TrainerT5.optimizer.zero_grad()\n",
    "        \n",
    "    class_keys = ds2.task_to_labels[task]\n",
    "    \n",
    "    val_acc, val_f1 = validate_lester(TrainerT5, dataloader_val, task,\n",
    "                                      embed_prompt=embed_prompt,\n",
    "                                      class_keys=class_keys,\n",
    "                                      max_length=target_len,\n",
    "                                      #print_outputs=True\n",
    "                                      ) # prompt tuning 5 \n",
    "   \n",
    "    print(epoch, '->', val_acc, val_f1)\n",
    "    #print('train acc ->', train_acc, train_f1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b978e679",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84e8339c9c2a4ceabccd4aa39bbe2333",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/409 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(0.6269113149847095, {'accuracy': 0.6269113149847095})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_acc, val_f1 = validate_lester(TrainerT5, dataloader_val, task,\n",
    "                                      embed_prompt=embed_prompt,\n",
    "                                      class_keys=class_keys,\n",
    "                                      max_length=target_len,\n",
    "                                      #print_outputs=True\n",
    "                                      ) # prompt tuning 5 \n",
    "val_acc, val_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db464754",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "80421d0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task =  boolq\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c72c52fc07b749069ce9e3eb51902b70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1179 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3564d75a8cf64103b29c2389dede7afb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/409 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad> true', '<pad> true', '<pad> true', '<pad> false', '<pad> true', '<pad> true', '<pad> true', '<pad> true']\n",
      "['true</s>', 'true</s>', 'false</s>', 'true</s>', 'false</s>', 'true</s>', 'true</s>', 'true</s>']\n",
      "['<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true']\n",
      "['false</s>', 'true</s>', 'true</s>', 'false</s>', 'true</s>', 'false</s>', 'true</s>', 'false</s>']\n",
      "['<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true']\n",
      "['false</s>', 'true</s>', 'false</s>', 'false</s>', 'false</s>', 'true</s>', 'true</s>', 'true</s>']\n",
      "['<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true']\n",
      "['false</s>', 'true</s>', 'false</s>', 'true</s>', 'true</s>', 'true</s>', 'true</s>', 'false</s>']\n",
      "['<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true']\n",
      "['false</s>', 'false</s>', 'true</s>', 'false</s>', 'true</s>', 'true</s>', 'true</s>', 'true</s>']\n",
      "['<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true']\n",
      "['true</s>', 'true</s>', 'false</s>', 'true</s>', 'true</s>', 'false</s>', 'true</s>', 'true</s>']\n",
      "['<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> false', '<pad> true', '<pad> true', '<pad> true']\n",
      "['false</s>', 'false</s>', 'false</s>', 'true</s>', 'false</s>', 'false</s>', 'true</s>', 'true</s>']\n",
      "['<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true']\n",
      "['false</s>', 'true</s>', 'false</s>', 'true</s>', 'true</s>', 'false</s>', 'false</s>', 'true</s>']\n",
      "['<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true']\n",
      "['true</s>', 'false</s>', 'true</s>', 'false</s>', 'false</s>', 'true</s>', 'false</s>', 'true</s>']\n",
      "['<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true']\n",
      "['false</s>', 'false</s>', 'true</s>', 'true</s>', 'false</s>', 'false</s>', 'true</s>', 'true</s>']\n",
      "0 -> 0.6311926605504588 0.327446\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e363aa5f55fd4c3682c5de76f4801634",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1179 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5eab9d2b5004305876ebdb1150a6922",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/409 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad> true', '<pad> true', '<pad> true', '<pad> false', '<pad> true', '<pad> true', '<pad> true', '<pad> true']\n",
      "['true</s>', 'true</s>', 'false</s>', 'true</s>', 'false</s>', 'true</s>', 'true</s>', 'true</s>']\n",
      "['<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true']\n",
      "['false</s>', 'true</s>', 'true</s>', 'false</s>', 'true</s>', 'false</s>', 'true</s>', 'false</s>']\n",
      "['<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> false', '<pad> true', '<pad> true', '<pad> true']\n",
      "['false</s>', 'true</s>', 'false</s>', 'false</s>', 'false</s>', 'true</s>', 'true</s>', 'true</s>']\n",
      "['<pad> true', '<pad> false', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true']\n",
      "['false</s>', 'true</s>', 'false</s>', 'true</s>', 'true</s>', 'true</s>', 'true</s>', 'false</s>']\n",
      "['<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> false', '<pad> true', '<pad> true', '<pad> true']\n",
      "['false</s>', 'false</s>', 'true</s>', 'false</s>', 'true</s>', 'true</s>', 'true</s>', 'true</s>']\n",
      "['<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true']\n",
      "['true</s>', 'true</s>', 'false</s>', 'true</s>', 'true</s>', 'false</s>', 'true</s>', 'true</s>']\n",
      "['<pad> true', '<pad> true', '<pad> false', '<pad> true', '<pad> false', '<pad> true', '<pad> true', '<pad> true']\n",
      "['false</s>', 'false</s>', 'false</s>', 'true</s>', 'false</s>', 'false</s>', 'true</s>', 'true</s>']\n",
      "['<pad> true', '<pad> true', '<pad> false', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> false']\n",
      "['false</s>', 'true</s>', 'false</s>', 'true</s>', 'true</s>', 'false</s>', 'false</s>', 'true</s>']\n",
      "['<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true']\n",
      "['true</s>', 'false</s>', 'true</s>', 'false</s>', 'false</s>', 'true</s>', 'false</s>', 'true</s>']\n",
      "['<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true']\n",
      "['false</s>', 'false</s>', 'true</s>', 'true</s>', 'false</s>', 'false</s>', 'true</s>', 'true</s>']\n",
      "1 -> 0.6247706422018349 0.33219868\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5313db01ab7c476787d3352021d08917",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1179 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9c7a8698c8042ee8003986c638a9c50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/409 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad> false', '<pad> true', '<pad> true', '<pad> false', '<pad> true', '<pad> true', '<pad> true', '<pad> true']\n",
      "['true</s>', 'true</s>', 'false</s>', 'true</s>', 'false</s>', 'true</s>', 'true</s>', 'true</s>']\n",
      "['<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true']\n",
      "['false</s>', 'true</s>', 'true</s>', 'false</s>', 'true</s>', 'false</s>', 'true</s>', 'false</s>']\n",
      "['<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true']\n",
      "['false</s>', 'true</s>', 'false</s>', 'false</s>', 'false</s>', 'true</s>', 'true</s>', 'true</s>']\n",
      "['<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> false', '<pad> false', '<pad> true']\n",
      "['false</s>', 'true</s>', 'false</s>', 'true</s>', 'true</s>', 'true</s>', 'true</s>', 'false</s>']\n",
      "['<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> false', '<pad> true', '<pad> true', '<pad> true']\n",
      "['false</s>', 'false</s>', 'true</s>', 'false</s>', 'true</s>', 'true</s>', 'true</s>', 'true</s>']\n",
      "['<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true']\n",
      "['true</s>', 'true</s>', 'false</s>', 'true</s>', 'true</s>', 'false</s>', 'true</s>', 'true</s>']\n",
      "['<pad> true', '<pad> true', '<pad> false', '<pad> true', '<pad> false', '<pad> true', '<pad> true', '<pad> true']\n",
      "['false</s>', 'false</s>', 'false</s>', 'true</s>', 'false</s>', 'false</s>', 'true</s>', 'true</s>']\n",
      "['<pad> true', '<pad> true', '<pad> false', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true']\n",
      "['false</s>', 'true</s>', 'false</s>', 'true</s>', 'true</s>', 'false</s>', 'false</s>', 'true</s>']\n",
      "['<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true']\n",
      "['true</s>', 'false</s>', 'true</s>', 'false</s>', 'false</s>', 'true</s>', 'false</s>', 'true</s>']\n",
      "['<pad> false', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true']\n",
      "['false</s>', 'false</s>', 'true</s>', 'true</s>', 'false</s>', 'false</s>', 'true</s>', 'true</s>']\n",
      "2 -> 0.6278287461773701 0.3304124\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e7ebdb2828a4ca3a7b2e64bc33cfc33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1179 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m batch \u001b[38;5;241m=\u001b[39m {k:batch[k]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m batch}\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#lim = batch['source_ids'].shape[0]\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step_lester\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTrainerT5\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTrainerT5\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membed_prompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     21\u001b[0m TrainerT5\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mtrain_step_lester\u001b[0;34m(trainer, batch, prompt, embed_prompt)\u001b[0m\n\u001b[1;32m     17\u001b[0m inputs_embeds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mconcat([prompt\u001b[38;5;241m.\u001b[39mrepeat(k, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     18\u001b[0m                               inputs_embeds], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[:,:\u001b[38;5;241m512\u001b[39m]\n\u001b[1;32m     20\u001b[0m source_mask_updated \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mconcat( (batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mrepeat(k,\u001b[38;5;241m5\u001b[39m),\n\u001b[1;32m     21\u001b[0m                                      batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[:,:\u001b[38;5;241m512\u001b[39m]\n\u001b[0;32m---> 23\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;66;43;03m#input_ids=batch[\"source_ids\"],\u001b[39;49;00m\n\u001b[1;32m     25\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource_mask_updated\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#batch[\"source_mask\"],\u001b[39;49;00m\n\u001b[1;32m     26\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;66;43;03m#labels=lm_labels,\u001b[39;49;00m\n\u001b[1;32m     27\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;66;43;03m#decoder_attention_mask=batch['target_mask']\u001b[39;49;00m\n\u001b[1;32m     28\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;66;43;03m#input_ids=input_ids,\u001b[39;49;00m\n\u001b[1;32m     29\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;66;43;03m#attention_mask=attention_mask,\u001b[39;49;00m\n\u001b[1;32m     30\u001b[0m \u001b[43m                        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#head_mask,\u001b[39;49;00m\n\u001b[1;32m     32\u001b[0m \u001b[43m                        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#output_attentions,\u001b[39;49;00m\n\u001b[1;32m     33\u001b[0m \u001b[43m                        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#output_hidden_states,\u001b[39;49;00m\n\u001b[1;32m     34\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#return_dict,\u001b[39;49;00m\n\u001b[1;32m     35\u001b[0m \u001b[43m                    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\n\u001b[1;32m     38\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     39\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39msource_mask_updated, \u001b[38;5;66;03m#batch[\"source_mask\"],\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     42\u001b[0m     encoder_outputs\u001b[38;5;241m=\u001b[39mencoder_outputs,\n\u001b[1;32m     43\u001b[0m )\n\u001b[1;32m     44\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda/envs/nlp/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py:1066\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1053\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m checkpoint(\n\u001b[1;32m   1054\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m   1055\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1063\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# past_key_value is always None with gradient checkpointing\u001b[39;00m\n\u001b[1;32m   1064\u001b[0m     )\n\u001b[1;32m   1065\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1066\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1067\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1068\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1069\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1070\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1071\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1072\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1073\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1074\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1075\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1076\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[38;5;66;03m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[1;32m   1081\u001b[0m \u001b[38;5;66;03m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[1;32m   1082\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda/envs/nlp/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py:695\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    692\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    693\u001b[0m     self_attn_past_key_value, cross_attn_past_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 695\u001b[0m self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    704\u001b[0m hidden_states, present_key_value_state \u001b[38;5;241m=\u001b[39m self_attention_outputs[:\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    705\u001b[0m attention_outputs \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m2\u001b[39m:]  \u001b[38;5;66;03m# Keep self-attention outputs and relative position weights\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/envs/nlp/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py:594\u001b[0m, in \u001b[0;36mT5LayerSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    584\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    586\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    592\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    593\u001b[0m ):\n\u001b[0;32m--> 594\u001b[0m     normed_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    595\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mSelfAttention(\n\u001b[1;32m    596\u001b[0m         normed_hidden_states,\n\u001b[1;32m    597\u001b[0m         mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    602\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    603\u001b[0m     )\n\u001b[1;32m    604\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madapter_layer_forward(hidden_states, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attention_output[\u001b[38;5;241m0\u001b[39m]), \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda/envs/nlp/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py:264\u001b[0m, in \u001b[0;36mT5LayerNorm.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states):\n\u001b[1;32m    258\u001b[0m \n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# T5 uses a layer_norm which only scales and doesn't shift, which is also known as Root Mean\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;66;03m# Square Layer Normalization https://arxiv.org/abs/1910.07467 thus varience is calculated\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;66;03m# w/o mean and there is no bias. Additionally we want to make sure that the accumulation for\u001b[39;00m\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;66;03m# half-precision inputs is done in fp32\u001b[39;00m\n\u001b[0;32m--> 264\u001b[0m     variance \u001b[38;5;241m=\u001b[39m \u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    265\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mrsqrt(variance \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariance_epsilon)\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;66;03m# convert into half-precision if necessary\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# after changing prompt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4ab6b82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6275229357798165, 0.33297685)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_acc, val_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a0e97f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "178e6351",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task =  boolq\n",
      "Using MLP?  False\n",
      "0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8548573e37184587868b24b661965b03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1179 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11b73dbe9bda45f2b3edd268ef11e021",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/409 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad> false', '<pad> true', '<pad> true', '<pad> false', '<pad> false', '<pad> true', '<pad> true', '<pad> false']\n",
      "['true</s>', 'true</s>', 'false</s>', 'true</s>', 'false</s>', 'true</s>', 'true</s>', 'true</s>']\n",
      "['<pad> true', '<pad> true', '<pad> false', '<pad> true', '<pad> true', '<pad> false', '<pad> true', '<pad> false']\n",
      "['false</s>', 'true</s>', 'true</s>', 'false</s>', 'true</s>', 'false</s>', 'true</s>', 'false</s>']\n",
      "['<pad> true', '<pad> false', '<pad> false', '<pad> true', '<pad> false', '<pad> false', '<pad> true', '<pad> false']\n",
      "['false</s>', 'true</s>', 'false</s>', 'false</s>', 'false</s>', 'true</s>', 'true</s>', 'true</s>']\n",
      "['<pad> true', '<pad> false', '<pad> false', '<pad> false', '<pad> true', '<pad> false', '<pad> false', '<pad> false']\n",
      "['false</s>', 'true</s>', 'false</s>', 'true</s>', 'true</s>', 'true</s>', 'true</s>', 'false</s>']\n",
      "['<pad> false', '<pad> true', '<pad> true', '<pad> false', '<pad> false', '<pad> true', '<pad> false', '<pad> true']\n",
      "['false</s>', 'false</s>', 'true</s>', 'false</s>', 'true</s>', 'true</s>', 'true</s>', 'true</s>']\n",
      "['<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> false', '<pad> true', '<pad> true', '<pad> true']\n",
      "['true</s>', 'true</s>', 'false</s>', 'true</s>', 'true</s>', 'false</s>', 'true</s>', 'true</s>']\n",
      "['<pad> false', '<pad> false', '<pad> false', '<pad> true', '<pad> false', '<pad> true', '<pad> true', '<pad> true']\n",
      "['false</s>', 'false</s>', 'false</s>', 'true</s>', 'false</s>', 'false</s>', 'true</s>', 'true</s>']\n",
      "['<pad> true', '<pad> false', '<pad> false', '<pad> false', '<pad> true', '<pad> false', '<pad> false', '<pad> false']\n",
      "['false</s>', 'true</s>', 'false</s>', 'true</s>', 'true</s>', 'false</s>', 'false</s>', 'true</s>']\n",
      "['<pad> true', '<pad> false', '<pad> false', '<pad> false', '<pad> true', '<pad> true', '<pad> false', '<pad> true']\n",
      "['true</s>', 'false</s>', 'true</s>', 'false</s>', 'false</s>', 'true</s>', 'false</s>', 'true</s>']\n",
      "['<pad> false', '<pad> true', '<pad> true', '<pad> false', '<pad> true', '<pad> false', '<pad> true', '<pad> false']\n",
      "['false</s>', 'false</s>', 'true</s>', 'true</s>', 'false</s>', 'false</s>', 'true</s>', 'true</s>']\n",
      "0 -> 0.5437308868501529 0.34407163\n",
      "1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce6e005f379b42b385c534cd71bfd8c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1179 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f613a33685ad4771943566eb60a8e15e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/409 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad> true', '<pad> true', '<pad> true', '<pad> false', '<pad> true', '<pad> true', '<pad> true', '<pad> true']\n",
      "['true</s>', 'true</s>', 'false</s>', 'true</s>', 'false</s>', 'true</s>', 'true</s>', 'true</s>']\n",
      "['<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true']\n",
      "['false</s>', 'true</s>', 'true</s>', 'false</s>', 'true</s>', 'false</s>', 'true</s>', 'false</s>']\n",
      "['<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true']\n",
      "['false</s>', 'true</s>', 'false</s>', 'false</s>', 'false</s>', 'true</s>', 'true</s>', 'true</s>']\n",
      "['<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true']\n",
      "['false</s>', 'true</s>', 'false</s>', 'true</s>', 'true</s>', 'true</s>', 'true</s>', 'false</s>']\n",
      "['<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true']\n",
      "['false</s>', 'false</s>', 'true</s>', 'false</s>', 'true</s>', 'true</s>', 'true</s>', 'true</s>']\n",
      "['<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true']\n",
      "['true</s>', 'true</s>', 'false</s>', 'true</s>', 'true</s>', 'false</s>', 'true</s>', 'true</s>']\n",
      "['<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> false', '<pad> true', '<pad> true', '<pad> true']\n",
      "['false</s>', 'false</s>', 'false</s>', 'true</s>', 'false</s>', 'false</s>', 'true</s>', 'true</s>']\n",
      "['<pad> true', '<pad> true', '<pad> false', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true']\n",
      "['false</s>', 'true</s>', 'false</s>', 'true</s>', 'true</s>', 'false</s>', 'false</s>', 'true</s>']\n",
      "['<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true']\n",
      "['true</s>', 'false</s>', 'true</s>', 'false</s>', 'false</s>', 'true</s>', 'false</s>', 'true</s>']\n",
      "['<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true']\n",
      "['false</s>', 'false</s>', 'true</s>', 'true</s>', 'false</s>', 'false</s>', 'true</s>', 'true</s>']\n",
      "1 -> 0.6290519877675841 0.32839623\n",
      "2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e4aa7466d9b4ba9848d23dd412c34c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1179 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2706db18c60a4eb398842a667e0f4d5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/409 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad> true', '<pad> true', '<pad> true', '<pad> false', '<pad> true', '<pad> true', '<pad> true', '<pad> true']\n",
      "['true</s>', 'true</s>', 'false</s>', 'true</s>', 'false</s>', 'true</s>', 'true</s>', 'true</s>']\n",
      "['<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true']\n",
      "['false</s>', 'true</s>', 'true</s>', 'false</s>', 'true</s>', 'false</s>', 'true</s>', 'false</s>']\n",
      "['<pad> true', '<pad> false', '<pad> false', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true']\n",
      "['false</s>', 'true</s>', 'false</s>', 'false</s>', 'false</s>', 'true</s>', 'true</s>', 'true</s>']\n",
      "['<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> false', '<pad> false', '<pad> true']\n",
      "['false</s>', 'true</s>', 'false</s>', 'true</s>', 'true</s>', 'true</s>', 'true</s>', 'false</s>']\n",
      "['<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true']\n",
      "['false</s>', 'false</s>', 'true</s>', 'false</s>', 'true</s>', 'true</s>', 'true</s>', 'true</s>']\n",
      "['<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true']\n",
      "['true</s>', 'true</s>', 'false</s>', 'true</s>', 'true</s>', 'false</s>', 'true</s>', 'true</s>']\n",
      "['<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> false', '<pad> true', '<pad> true', '<pad> true']\n",
      "['false</s>', 'false</s>', 'false</s>', 'true</s>', 'false</s>', 'false</s>', 'true</s>', 'true</s>']\n",
      "['<pad> true', '<pad> true', '<pad> false', '<pad> true', '<pad> true', '<pad> false', '<pad> true', '<pad> true']\n",
      "['false</s>', 'true</s>', 'false</s>', 'true</s>', 'true</s>', 'false</s>', 'false</s>', 'true</s>']\n",
      "['<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true']\n",
      "['true</s>', 'false</s>', 'true</s>', 'false</s>', 'false</s>', 'true</s>', 'false</s>', 'true</s>']\n",
      "['<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true']\n",
      "['false</s>', 'false</s>', 'true</s>', 'true</s>', 'false</s>', 'false</s>', 'true</s>', 'true</s>']\n",
      "2 -> 0.6241590214067279 0.32954583\n",
      "3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8f40d57f606496ca19e3a549fb01ab9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1179 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b1c4f2e30dd4ccc8dd9b31b8a0094b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/409 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad> true', '<pad> true', '<pad> true', '<pad> false', '<pad> true', '<pad> true', '<pad> true', '<pad> true']\n",
      "['true</s>', 'true</s>', 'false</s>', 'true</s>', 'false</s>', 'true</s>', 'true</s>', 'true</s>']\n",
      "['<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true']\n",
      "['false</s>', 'true</s>', 'true</s>', 'false</s>', 'true</s>', 'false</s>', 'true</s>', 'false</s>']\n",
      "['<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true']\n",
      "['false</s>', 'true</s>', 'false</s>', 'false</s>', 'false</s>', 'true</s>', 'true</s>', 'true</s>']\n",
      "['<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true']\n",
      "['false</s>', 'true</s>', 'false</s>', 'true</s>', 'true</s>', 'true</s>', 'true</s>', 'false</s>']\n",
      "['<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true']\n",
      "['false</s>', 'false</s>', 'true</s>', 'false</s>', 'true</s>', 'true</s>', 'true</s>', 'true</s>']\n",
      "['<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true']\n",
      "['true</s>', 'true</s>', 'false</s>', 'true</s>', 'true</s>', 'false</s>', 'true</s>', 'true</s>']\n",
      "['<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> false', '<pad> true', '<pad> true', '<pad> true']\n",
      "['false</s>', 'false</s>', 'false</s>', 'true</s>', 'false</s>', 'false</s>', 'true</s>', 'true</s>']\n",
      "['<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true']\n",
      "['false</s>', 'true</s>', 'false</s>', 'true</s>', 'true</s>', 'false</s>', 'false</s>', 'true</s>']\n",
      "['<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true']\n",
      "['true</s>', 'false</s>', 'true</s>', 'false</s>', 'false</s>', 'true</s>', 'false</s>', 'true</s>']\n",
      "['<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true']\n",
      "['false</s>', 'false</s>', 'true</s>', 'true</s>', 'false</s>', 'false</s>', 'true</s>', 'true</s>']\n",
      "3 -> 0.6327217125382263 0.32594997\n",
      "4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8e8b85975514c0fa6fc4b05dc2646b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1179 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1989af5b0344fa6b15d1152fbf60cd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/409 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad> true', '<pad> true', '<pad> true', '<pad> false', '<pad> true', '<pad> true', '<pad> true', '<pad> true']\n",
      "['true</s>', 'true</s>', 'false</s>', 'true</s>', 'false</s>', 'true</s>', 'true</s>', 'true</s>']\n",
      "['<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> false', '<pad> true', '<pad> true']\n",
      "['false</s>', 'true</s>', 'true</s>', 'false</s>', 'true</s>', 'false</s>', 'true</s>', 'false</s>']\n",
      "['<pad> true', '<pad> false', '<pad> false', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true']\n",
      "['false</s>', 'true</s>', 'false</s>', 'false</s>', 'false</s>', 'true</s>', 'true</s>', 'true</s>']\n",
      "['<pad> true', '<pad> false', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true']\n",
      "['false</s>', 'true</s>', 'false</s>', 'true</s>', 'true</s>', 'true</s>', 'true</s>', 'false</s>']\n",
      "['<pad> false', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true']\n",
      "['false</s>', 'false</s>', 'true</s>', 'false</s>', 'true</s>', 'true</s>', 'true</s>', 'true</s>']\n",
      "['<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true']\n",
      "['true</s>', 'true</s>', 'false</s>', 'true</s>', 'true</s>', 'false</s>', 'true</s>', 'true</s>']\n",
      "['<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> false', '<pad> true', '<pad> true', '<pad> true']\n",
      "['false</s>', 'false</s>', 'false</s>', 'true</s>', 'false</s>', 'false</s>', 'true</s>', 'true</s>']\n",
      "['<pad> true', '<pad> true', '<pad> false', '<pad> true', '<pad> true', '<pad> false', '<pad> true', '<pad> true']\n",
      "['false</s>', 'true</s>', 'false</s>', 'true</s>', 'true</s>', 'false</s>', 'false</s>', 'true</s>']\n",
      "['<pad> true', '<pad> true', '<pad> true', '<pad> false', '<pad> true', '<pad> true', '<pad> true', '<pad> true']\n",
      "['true</s>', 'false</s>', 'true</s>', 'false</s>', 'false</s>', 'true</s>', 'false</s>', 'true</s>']\n",
      "['<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> false', '<pad> true', '<pad> true']\n",
      "['false</s>', 'false</s>', 'true</s>', 'true</s>', 'false</s>', 'false</s>', 'true</s>', 'true</s>']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     TrainerT5\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     21\u001b[0m     TrainerT5\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 23\u001b[0m acc, loss \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_lester\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTrainerT5\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mclass_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mprint_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# prompt tuning 5\u001b[39;00m\n\u001b[1;32m     28\u001b[0m results_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124macc\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(acc)\n\u001b[1;32m     29\u001b[0m results_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(loss)\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mvalidate_lester\u001b[0;34m(trainer, dataloader_val, task, embed_prompt, class_keys, max_length, print_outputs)\u001b[0m\n\u001b[1;32m     99\u001b[0m outs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m    100\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;66;03m#attention_mask=batch[\"source_mask\"],\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    106\u001b[0m     max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[1;32m    107\u001b[0m )\n\u001b[1;32m    108\u001b[0m dec \u001b[38;5;241m=\u001b[39m [tokenizer\u001b[38;5;241m.\u001b[39mdecode(ids) \u001b[38;5;28;01mfor\u001b[39;00m ids \u001b[38;5;129;01min\u001b[39;00m outs]\n\u001b[0;32m--> 109\u001b[0m texts \u001b[38;5;241m=\u001b[39m [tokenizer\u001b[38;5;241m.\u001b[39mdecode(ids) \u001b[38;5;28;01mfor\u001b[39;00m ids \u001b[38;5;129;01min\u001b[39;00m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m    110\u001b[0m targets \u001b[38;5;241m=\u001b[39m [tokenizer\u001b[38;5;241m.\u001b[39mdecode(ids) \u001b[38;5;28;01mfor\u001b[39;00m ids \u001b[38;5;129;01min\u001b[39;00m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m#print(dec, texts, targets)\u001b[39;00m\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     99\u001b[0m outs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m    100\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;66;03m#attention_mask=batch[\"source_mask\"],\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    106\u001b[0m     max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[1;32m    107\u001b[0m )\n\u001b[1;32m    108\u001b[0m dec \u001b[38;5;241m=\u001b[39m [tokenizer\u001b[38;5;241m.\u001b[39mdecode(ids) \u001b[38;5;28;01mfor\u001b[39;00m ids \u001b[38;5;129;01min\u001b[39;00m outs]\n\u001b[0;32m--> 109\u001b[0m texts \u001b[38;5;241m=\u001b[39m [\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mids\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m ids \u001b[38;5;129;01min\u001b[39;00m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m    110\u001b[0m targets \u001b[38;5;241m=\u001b[39m [tokenizer\u001b[38;5;241m.\u001b[39mdecode(ids) \u001b[38;5;28;01mfor\u001b[39;00m ids \u001b[38;5;129;01min\u001b[39;00m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m#print(dec, texts, targets)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/envs/nlp/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3312\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3309\u001b[0m \u001b[38;5;66;03m# Convert inputs to python lists\u001b[39;00m\n\u001b[1;32m   3310\u001b[0m token_ids \u001b[38;5;241m=\u001b[39m to_py_obj(token_ids)\n\u001b[0;32m-> 3312\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3315\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3316\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3317\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/nlp/lib/python3.9/site-packages/transformers/tokenization_utils.py:947\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, spaces_between_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    945\u001b[0m         current_sub_text\u001b[38;5;241m.\u001b[39mappend(token)\n\u001b[1;32m    946\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m current_sub_text:\n\u001b[0;32m--> 947\u001b[0m     sub_texts\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_tokens_to_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_sub_text\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    949\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spaces_between_special_tokens:\n\u001b[1;32m    950\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(sub_texts)\n",
      "File \u001b[0;32m~/miniconda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:290\u001b[0m, in \u001b[0;36mT5Tokenizer.convert_tokens_to_string\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    287\u001b[0m out_string \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens:\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;66;03m# make sure that special tokens are not decoded using sentencepiece model\u001b[39;00m\n\u001b[0;32m--> 290\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_special_tokens\u001b[49m:\n\u001b[1;32m    291\u001b[0m         out_string \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msp_model\u001b[38;5;241m.\u001b[39mdecode_pieces(current_sub_tokens) \u001b[38;5;241m+\u001b[39m token \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    292\u001b[0m         current_sub_tokens \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda/envs/nlp/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1230\u001b[0m, in \u001b[0;36mSpecialTokensMixin.all_special_tokens\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1223\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m   1224\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mall_special_tokens\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m   1225\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1226\u001b[0m \u001b[38;5;124;03m    `List[str]`: All the special tokens (`'<unk>'`, `'<cls>'`, etc.) mapped to class attributes.\u001b[39;00m\n\u001b[1;32m   1227\u001b[0m \n\u001b[1;32m   1228\u001b[0m \u001b[38;5;124;03m    Convert tokens of `tokenizers.AddedToken` type to string.\u001b[39;00m\n\u001b[1;32m   1229\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1230\u001b[0m     all_toks \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mstr\u001b[39m(s) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_special_tokens_extended]\n\u001b[1;32m   1231\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m all_toks\n",
      "File \u001b[0;32m~/miniconda/envs/nlp/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1230\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1223\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m   1224\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mall_special_tokens\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m   1225\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1226\u001b[0m \u001b[38;5;124;03m    `List[str]`: All the special tokens (`'<unk>'`, `'<cls>'`, etc.) mapped to class attributes.\u001b[39;00m\n\u001b[1;32m   1227\u001b[0m \n\u001b[1;32m   1228\u001b[0m \u001b[38;5;124;03m    Convert tokens of `tokenizers.AddedToken` type to string.\u001b[39;00m\n\u001b[1;32m   1229\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1230\u001b[0m     all_toks \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_special_tokens_extended]\n\u001b[1;32m   1231\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m all_toks\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('task = ', task)\n",
    "model = TrainerT5.model\n",
    "model.to('cuda')\n",
    "\n",
    "embed_prompt = False # NO PREFIX MLP\n",
    "print(\"Using MLP? \", embed_prompt)\n",
    "\n",
    "results_dict = {'acc': [], 'loss': []}\n",
    "\n",
    "for epoch in range(20):\n",
    "    print(epoch)\n",
    "    model.train()\n",
    "    #mlp.train()\n",
    "\n",
    "    for i, batch in enumerate(tqdm(dataloader_train)):\n",
    "        batch = {k:batch[k].to('cuda') for k in batch}\n",
    "        loss = train_step_lester(TrainerT5, batch, TrainerT5.model.prompt, embed_prompt=embed_prompt)\n",
    "        loss.backward()\n",
    "\n",
    "        TrainerT5.optimizer.step()\n",
    "        TrainerT5.optimizer.zero_grad()\n",
    "\n",
    "    acc, loss = validate_lester(TrainerT5, dataloader_val,\n",
    "                                task, embed_prompt,\n",
    "                                class_keys=class_keys,\n",
    "                                max_length=target_len,\n",
    "                                print_outputs=True) # prompt tuning 5\n",
    "    results_dict['acc'].append(acc)\n",
    "    results_dict['loss'].append(loss)\n",
    "    print(epoch, '->', acc, loss)\n",
    "    #print('train acc ->', train_acc, train_f1)\n",
    "\n",
    "#     if save_path!=None and epoch%5==0:\n",
    "#         np.save(os.path.join(save_path, 'results_dict.npy'), results_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c6d00d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aadc8ab9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task =  boolq\n",
      "Using MLP?  False\n",
      "0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77c5bdca50fa424bbe298424e7ce3ee8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1179 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd5d1070e7b447d0931751585315dd66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/409 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true']\n",
      "['true</s>', 'true</s>', 'false</s>', 'true</s>', 'false</s>', 'true</s>', 'true</s>', 'true</s>']\n",
      "['<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true']\n",
      "['false</s>', 'true</s>', 'true</s>', 'false</s>', 'true</s>', 'false</s>', 'true</s>', 'false</s>']\n",
      "['<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true']\n",
      "['false</s>', 'true</s>', 'false</s>', 'false</s>', 'false</s>', 'true</s>', 'true</s>', 'true</s>']\n",
      "['<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true']\n",
      "['false</s>', 'true</s>', 'false</s>', 'true</s>', 'true</s>', 'true</s>', 'true</s>', 'false</s>']\n",
      "['<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true']\n",
      "['false</s>', 'false</s>', 'true</s>', 'false</s>', 'true</s>', 'true</s>', 'true</s>', 'true</s>']\n",
      "['<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true']\n",
      "['true</s>', 'true</s>', 'false</s>', 'true</s>', 'true</s>', 'false</s>', 'true</s>', 'true</s>']\n",
      "['<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> false', '<pad> true', '<pad> true', '<pad> true']\n",
      "['false</s>', 'false</s>', 'false</s>', 'true</s>', 'false</s>', 'false</s>', 'true</s>', 'true</s>']\n",
      "['<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true']\n",
      "['false</s>', 'true</s>', 'false</s>', 'true</s>', 'true</s>', 'false</s>', 'false</s>', 'true</s>']\n",
      "['<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true']\n",
      "['true</s>', 'false</s>', 'true</s>', 'false</s>', 'false</s>', 'true</s>', 'false</s>', 'true</s>']\n",
      "['<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true']\n",
      "['false</s>', 'false</s>', 'true</s>', 'true</s>', 'false</s>', 'false</s>', 'true</s>', 'true</s>']\n",
      "0 -> 0.6238532110091743 0.32963803\n",
      "1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9eaec6adf144a05ada273eae6fe9581",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1179 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b419163144e5426fb406e98e339ac7b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/409 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad> true', '<pad> true', '<pad> true', '<pad> false', '<pad> true', '<pad> true', '<pad> true', '<pad> true']\n",
      "['true</s>', 'true</s>', 'false</s>', 'true</s>', 'false</s>', 'true</s>', 'true</s>', 'true</s>']\n",
      "['<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true']\n",
      "['false</s>', 'true</s>', 'true</s>', 'false</s>', 'true</s>', 'false</s>', 'true</s>', 'false</s>']\n",
      "['<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true']\n",
      "['false</s>', 'true</s>', 'false</s>', 'false</s>', 'false</s>', 'true</s>', 'true</s>', 'true</s>']\n",
      "['<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true']\n",
      "['false</s>', 'true</s>', 'false</s>', 'true</s>', 'true</s>', 'true</s>', 'true</s>', 'false</s>']\n",
      "['<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true']\n",
      "['false</s>', 'false</s>', 'true</s>', 'false</s>', 'true</s>', 'true</s>', 'true</s>', 'true</s>']\n",
      "['<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true']\n",
      "['true</s>', 'true</s>', 'false</s>', 'true</s>', 'true</s>', 'false</s>', 'true</s>', 'true</s>']\n",
      "['<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> false', '<pad> true', '<pad> true', '<pad> true']\n",
      "['false</s>', 'false</s>', 'false</s>', 'true</s>', 'false</s>', 'false</s>', 'true</s>', 'true</s>']\n",
      "['<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true']\n",
      "['false</s>', 'true</s>', 'false</s>', 'true</s>', 'true</s>', 'false</s>', 'false</s>', 'true</s>']\n",
      "['<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true']\n",
      "['true</s>', 'false</s>', 'true</s>', 'false</s>', 'false</s>', 'true</s>', 'false</s>', 'true</s>']\n",
      "['<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true', '<pad> true']\n",
      "['false</s>', 'false</s>', 'true</s>', 'true</s>', 'false</s>', 'false</s>', 'true</s>', 'true</s>']\n",
      "1 -> 0.6287461773700306 0.32886517\n",
      "2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b084445cd7664831931a6b6f6fbbda91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1179 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(dataloader_train)):\n\u001b[1;32m     16\u001b[0m     batch \u001b[38;5;241m=\u001b[39m {k:batch[k]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m batch}\n\u001b[0;32m---> 17\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step_lester\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTrainerT5\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTrainerT5\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membed_prompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     20\u001b[0m     TrainerT5\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mtrain_step_lester\u001b[0;34m(trainer, batch, prompt, embed_prompt)\u001b[0m\n\u001b[1;32m     21\u001b[0m source_mask_updated \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mconcat( (batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mrepeat(k,\u001b[38;5;241m5\u001b[39m),\n\u001b[1;32m     22\u001b[0m                                      batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[:,:\u001b[38;5;241m512\u001b[39m]\n\u001b[1;32m     24\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[1;32m     25\u001b[0m                         \u001b[38;5;66;03m#input_ids=batch[\"source_ids\"],\u001b[39;00m\n\u001b[1;32m     26\u001b[0m                         attention_mask\u001b[38;5;241m=\u001b[39msource_mask_updated, \u001b[38;5;66;03m#batch[\"source_mask\"],\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m                         return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;66;03m#return_dict,\u001b[39;00m\n\u001b[1;32m     36\u001b[0m                     )\n\u001b[0;32m---> 38\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msource_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource_mask_updated\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#batch[\"source_mask\"],\u001b[39;49;00m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlm_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtarget_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/miniconda/envs/nlp/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda/envs/nlp/lib/python3.9/site-packages/transformers/adapters/context.py:103\u001b[0m, in \u001b[0;36mForwardContext.wrap.<locals>.wrapper_func\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39madapters \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 103\u001b[0m         results \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py:1692\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1689\u001b[0m         decoder_attention_mask \u001b[38;5;241m=\u001b[39m decoder_attention_mask\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mfirst_device)\n\u001b[1;32m   1691\u001b[0m \u001b[38;5;66;03m# Decode\u001b[39;00m\n\u001b[0;32m-> 1692\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1693\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1694\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1695\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1696\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1697\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1698\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1699\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1700\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1701\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1702\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1703\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1704\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1705\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1707\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m decoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1709\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/envs/nlp/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda/envs/nlp/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py:989\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    985\u001b[0m     past_key_values \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock)\n\u001b[1;32m    987\u001b[0m \u001b[38;5;66;03m# We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\u001b[39;00m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;66;03m# ourselves in which case we just need to make it broadcastable to all heads.\u001b[39;00m\n\u001b[0;32m--> 989\u001b[0m extended_attention_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_extended_attention_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    991\u001b[0m \u001b[38;5;66;03m# If a 2D or 3D attention mask is provided for the cross-attention\u001b[39;00m\n\u001b[1;32m    992\u001b[0m \u001b[38;5;66;03m# we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\u001b[39;00m\n\u001b[1;32m    993\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_decoder \u001b[38;5;129;01mand\u001b[39;00m encoder_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda/envs/nlp/lib/python3.9/site-packages/transformers/modeling_utils.py:310\u001b[0m, in \u001b[0;36mModuleUtilsMixin.get_extended_attention_mask\u001b[0;34m(self, attention_mask, input_shape, device)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    302\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrong shape for input_ids (shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) or attention_mask (shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattention_mask\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    303\u001b[0m     )\n\u001b[1;32m    305\u001b[0m \u001b[38;5;66;03m# Since attention_mask is 1.0 for positions we want to attend and 0.0 for\u001b[39;00m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;66;03m# masked positions, this operation will create a tensor which is 0.0 for\u001b[39;00m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;66;03m# positions we want to attend and -10000.0 for masked positions.\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;66;03m# Since we are adding it to the raw scores before the softmax, this is\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;66;03m# effectively the same as removing these entirely.\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m extended_attention_mask \u001b[38;5;241m=\u001b[39m extended_attention_mask\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m)  \u001b[38;5;66;03m# fp16 compatibility\u001b[39;00m\n\u001b[1;32m    311\u001b[0m extended_attention_mask \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m extended_attention_mask) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m10000.0\u001b[39m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m extended_attention_mask\n",
      "File \u001b[0;32m~/miniconda/envs/nlp/lib/python3.9/site-packages/transformers/modeling_utils.py:216\u001b[0m, in \u001b[0;36mModuleUtilsMixin.dtype\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdtype\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mdtype:\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;124;03m    `torch.dtype`: The dtype of the module (assuming that all the module parameters have the same dtype).\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_parameter_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/nlp/lib/python3.9/site-packages/transformers/modeling_utils.py:138\u001b[0m, in \u001b[0;36mget_parameter_dtype\u001b[0;34m(parameter)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_parameter_dtype\u001b[39m(parameter: Union[nn\u001b[38;5;241m.\u001b[39mModule, GenerationMixin, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModuleUtilsMixin\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 138\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparameter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;66;03m# For nn.DataParallel compatibility in PyTorch 1.5\u001b[39;00m\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_tensor_attributes\u001b[39m(module: nn\u001b[38;5;241m.\u001b[39mModule) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Tuple[\u001b[38;5;28mstr\u001b[39m, Tensor]]:\n",
      "File \u001b[0;32m~/miniconda/envs/nlp/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule.parameters\u001b[0;34m(self, recurse)\u001b[0m\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparameters\u001b[39m(\u001b[38;5;28mself\u001b[39m, recurse: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Parameter]:\n\u001b[1;32m   1500\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Returns an iterator over module parameters.\u001b[39;00m\n\u001b[1;32m   1501\u001b[0m \n\u001b[1;32m   1502\u001b[0m \u001b[38;5;124;03m    This is typically passed to an optimizer.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1518\u001b[0m \n\u001b[1;32m   1519\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnamed_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecurse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrecurse\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1521\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m param\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# with bug (prompt copied to prompt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af6a7d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -> 0.6220183486238532 0.33131248\n"
     ]
    }
   ],
   "source": [
    "print(epoch, '->', acc, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc4d871",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
